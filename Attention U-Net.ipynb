{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f3e68c",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;line-height:1.5em;font-size:30px;\">Data and Scripts <br>for Hydrological streamline Detection<br> Using U-NET Attention Module</h1>\n",
    "\n",
    "<p style=\"text-align:center;font-size:12px;\">\n",
    "<strong>Department of Geography and Environmental Resources, Southern Illinois University Carbondale, IL, USA</strong><br>\n",
    "<strong>Reference: ùëçùëíùë§ùëíùëñ ùëãùë¢ et. al. 2021</strong><br>\n",
    "</p>\n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the dependencies\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "np.random.seed(1337) # for reproducibility\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(1337)\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from itertools import chain\n",
    "from keras.layers import Layer,UpSampling2D,concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.core import SpatialDropout2D, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers import Reshape, Permute, Input, add, multiply\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.layers import dot\n",
    "# To specify the GPU ID uncomment this block and  \n",
    "# with K.tf.device('/gpu:0'): # specify the ID of GPU here (0: the first GPU)\n",
    "#    config = tf.ConfigProto(intra_op_parallelism_threads=4,\\\n",
    "#           inter_op_parallelism_threads=4, allow_soft_placement=True,\\\n",
    "#           device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "#    session = tf.Session(config=config)\n",
    "#    K.set_session(session)\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b2b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dice coefficient function as the loss function \n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(K.cast(y_true, 'float32'))\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
    "\n",
    "# Jacard coefficient\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "# calculate loss value\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)\n",
    "\n",
    "# calculate loss value\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c642dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Residual_CNN_block(x, size, dropout=0.0, batch_norm=True):\n",
    "    if K.image_data_format() == 'th':\n",
    "        axis = 1\n",
    "    else:\n",
    "        axis = 3\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    return conv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d265b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiplication(Layer):\n",
    "    def __init__(self,inter_channel = None,**kwargs):\n",
    "        super(multiplication, self).__init__(**kwargs)\n",
    "        self.inter_channel = inter_channel\n",
    "    def build(self,input_shape=None):\n",
    "        self.k = self.add_weight(name='k',shape=(1,),initializer='zeros',dtype='float32',trainable=True)\n",
    "    def get_config(self):\n",
    "        base_config = super(multiplication, self).get_config()\n",
    "        config = {'inter_channel':self.inter_channel}\n",
    "        return dict(list(base_config.items()) + list(config.items()))  \n",
    "    def call(self,inputs):\n",
    "        g,x,x_query,phi_g,x_value = inputs[0],inputs[1],inputs[2],inputs[3],inputs[4]\n",
    "        h,w,c = int(x.shape[1]),int(x.shape[2]),int(x.shape[3])\n",
    "        x_query = K.reshape(x_query, shape=(-1,h*w, self.inter_channel//4))\n",
    "        phi_g = K.reshape(phi_g,shape=(-1,h*w,self.inter_channel//4))\n",
    "        x_value = K.reshape(x_value,shape=(-1,h*w,c))\n",
    "        scale = dot([K.permute_dimensions(phi_g,(0,2,1)), x_query], axes=(1, 2))\n",
    "        soft_scale = Activation('softmax')(scale)\n",
    "        scaled_value = dot([K.permute_dimensions(soft_scale,(0,2,1)),K.permute_dimensions(x_value,(0,2,1))],axes=(1, 2))\n",
    "        scaled_value = K.reshape(scaled_value, shape=(-1,h,w,c))        \n",
    "        customize_multi = self.k * scaled_value\n",
    "        layero = add([customize_multi,x])\n",
    "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
    "        concate = my_concat([layero,g])\n",
    "        return concate \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        ll = list(input_shape)[1]\n",
    "        return (None,ll[1],ll[1],ll[3]*3)\n",
    "    def get_custom_objects():\n",
    "        return {'multiplication': multiplication}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb469560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_up_and_concatenate(inputs):\n",
    "    g,x = inputs[0],inputs[1]\n",
    "    inter_channel = g.get_shape().as_list()[3]\n",
    "    g = Conv2DTranspose(inter_channel, (2,2), strides=[2, 2],padding='same')(g)\n",
    "    x_query = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    phi_g = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    x_value = Conv2D(inter_channel//2, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    inputs = [g,x,x_query,phi_g,x_value]\n",
    "    concate = multiplication(inter_channel)(inputs)\n",
    "    return concate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda42219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiplication2(Layer):\n",
    "    def __init__(self,inter_channel = None,**kwargs):\n",
    "        super(multiplication2, self).__init__(**kwargs)\n",
    "        self.inter_channel = inter_channel\n",
    "    def build(self,input_shape=None):\n",
    "        self.k = self.add_weight(name='k',shape=(1,),initializer='zeros',dtype='float32',trainable=True)\n",
    "    def get_config(self):\n",
    "        base_config = super(multiplication2, self).get_config()\n",
    "        config = {'inter_channel':self.inter_channel}\n",
    "        return dict(list(base_config.items()) + list(config.items()))  \n",
    "    def call(self,inputs):\n",
    "        g,x,rate = inputs[0],inputs[1],inputs[2]\n",
    "        scaled_value = multiply([x, rate])\n",
    "        att_x =  self.k * scaled_value\n",
    "        att_x = add([att_x,x])\n",
    "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
    "        concate = my_concat([att_x, g])\n",
    "        return concate \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        ll = list(input_shape)[1]\n",
    "        return (None,ll[1],ll[1],ll[3]*2)\n",
    "    def get_custom_objects():\n",
    "        return {'multiplication2': multiplication2}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_up_and_concatenate2(inputs):\n",
    "    g, x = inputs[0],inputs[1]\n",
    "    inter_channel = g.get_shape().as_list()[3]\n",
    "    g = Conv2DTranspose(inter_channel//2, (3,3), strides=[2, 2],padding='same')(g)\n",
    "    g = Conv2D(inter_channel//2, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    theta_x = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    phi_g = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    f = Activation('relu')(add([theta_x, phi_g]))\n",
    "    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format='channels_last')(f)\n",
    "    rate = Activation('sigmoid')(psi_f)\n",
    "    concate =  multiplication2()([g,x,rate])\n",
    "    return concate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15530109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNET_224(weights=None):\n",
    "    inputs = Input((IMG_WIDTH, IMG_WIDTH, INPUT_CHANNELS))\n",
    "    filters = 32\n",
    "    last_dropout = 0.2\n",
    "# convolutiona and pooling level 1\n",
    "    conv_224 = Residual_CNN_block(inputs,filters)\n",
    "    pool_112 = MaxPooling2D(pool_size=(2, 2))(conv_224)\n",
    "# convolutiona and pooling level 2\n",
    "    conv_112 = Residual_CNN_block(pool_112,2*filters)\n",
    "    pool_56 = MaxPooling2D(pool_size=(2, 2))(conv_112)\n",
    "# convolutiona and pooling level 3\n",
    "    conv_56 = Residual_CNN_block(pool_56,4*filters)\n",
    "    pool_28 = MaxPooling2D(pool_size=(2, 2))(conv_56)\n",
    "# convolutiona and pooling level 4\n",
    "    conv_28 = Residual_CNN_block(pool_28,8*filters)\n",
    "    pool_14 = MaxPooling2D(pool_size=(2, 2))(conv_28)\n",
    "# convolutiona and pooling level 5\n",
    "    conv_14 = Residual_CNN_block(pool_14,16*filters)\n",
    "    pool_7 = MaxPooling2D(pool_size=(2, 2))(conv_14)\n",
    "# Conlovlution and feature concatenation\n",
    "    conv_7 = Residual_CNN_block(pool_7,32*filters)\n",
    "# Upsampling with convolution \n",
    "    up_14 = attention_up_and_concatenate([conv_7, conv_14]) \n",
    "    up_conv_14 = Residual_CNN_block(up_14,16*filters)\n",
    "# Upsampling with convolution 2\n",
    "    up_28 = attention_up_and_concatenate([up_conv_14, conv_28])\n",
    "    up_conv_28 = Residual_CNN_block(up_28,8*filters)\n",
    "# Upsampling with convolution 3\n",
    "    up_56 = attention_up_and_concatenate2([up_conv_28, conv_56])\n",
    "    up_conv_56 = Residual_CNN_block(up_56,4*filters)\n",
    "# Upsampling with convolution 4\n",
    "    up_112 = attention_up_and_concatenate2([up_conv_56, conv_112])\n",
    "    up_conv_112 = Residual_CNN_block(up_112,2*filters)\n",
    "# Upsampling with convolution 5\n",
    "    up_224 = attention_up_and_concatenate2([up_conv_112, conv_224])\n",
    "    #up_224 = attention_up_and_concatenate2(up_conv_112, conv_224)\n",
    "    up_conv_224 = Residual_CNN_block(up_224,filters,dropout = last_dropout)\n",
    "# 1 dimensional convolution and generate probabilities from Sigmoid function\n",
    "    conv_final = Conv2D(OUTPUT_MASK_CHANNELS, (1, 1))(up_conv_224)  \n",
    "    conv_final = Activation('sigmoid')(conv_final)\n",
    "# Generate model\n",
    "    model = Model(inputs, conv_final, name=\"UNET_224\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64458465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# scenario = raw_input(\"Please specify the scenario (up,down,left,or right):\")\n",
    "# aug = v2\n",
    "# read in training and validation data\n",
    "train_data = np.load(r\"C:\\Users\\mikeb\\Desktop\\New data\\train2\\data\\train_data_augv2.npy\")#[:2000]\n",
    "train_label = np.load(r\"C:\\Users\\mikeb\\Desktop\\New data\\train2\\data\\train_label_augv2.npy\")#[:,:,:,np.newaxis]#[:,:,:,np.newaxis]#[:2000]\n",
    "# X_Validation = np.load(r\"C:\\Users\\mikeb\\Desktop\\orig\\data\\train_data_augv2.npy\")[:700]\n",
    "# Y_Validation = np.load(r\"C:\\Users\\mikeb\\Desktop\\orig\\data\\train_label_augv2.npy\")#[:,:,:,np.newaxis][:700]\n",
    "# np.save(r\"C:\\Users\\mikeb\\Desktop\\orig\\train_label_augP2_.npy\",Y_Validation[:,:,:,np.newaxis])\n",
    "\n",
    "\n",
    "\n",
    "# Assuming your original training data and labels are stored in variables `train_data` and `train_label`\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Reshape the training labels to match the desired split shape\n",
    "# train_label = train_label.reshape(245, -1)\n",
    "\n",
    "# Split the training data and labels into training and validation sets\n",
    "train_data, vali_data, train_label, vali_label = train_test_split(train_data, train_label, test_size=20, random_state=42)\n",
    "\n",
    "# Verify the shapes of the training and validation sets\n",
    "print(train_data.shape)  # (225, 224, 224, 5)\n",
    "print(vali_data.shape)  # (20, 224, 224, 5)\n",
    "print(train_label.shape)  # (225, 224, 224, 1)\n",
    "print(vali_label.shape)  # (20, 224, 224, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "s = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(s)\n",
    "train_data = train_data[s]\n",
    "train_label = train_label[s]\n",
    "\n",
    "# print(X_train.shape) \n",
    "# print(Y_train.shape)\n",
    "# print(X_Validation.shape)\n",
    "# print(Y_Validation.shape)\n",
    "\n",
    "patch_size = 224\n",
    "\n",
    "IMG_WIDTH = patch_size\n",
    "IMG_HEIGHT = patch_size\n",
    "# Number of feature channels \n",
    "INPUT_CHANNELS = 5\n",
    "# Number of output masks (1 in case you predict only one type of objects)\n",
    "OUTPUT_MASK_CHANNELS = 1\n",
    "maxepoch = 300\n",
    "# hyperparameters\n",
    "learning_rate =0.0000359\n",
    "patience = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5909cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# scenario = raw_input(\"Please specify the scenario (up,down,left,or right):\")\n",
    "# aug = '_aug_'+scenario \n",
    "# read in training and validation data\n",
    "X_train = np.load(r\"C:\\Users\\mikeb\\Desktop\\di_gen\\data\\train_data_augv2.npy\")#[:2000]\n",
    "Y_train = np.load(r\"C:\\Users\\mikeb\\Desktop\\di_gen\\data\\train_label_augv2.npy\")#[:,:,:,np.newaxis]#[:,:,:,np.newaxis]#[:2000]\n",
    "X_Validation = np.load(r\"C:\\Users\\mikeb\\Desktop\\di_gen\\valid\\data\\train_data_augv2.npy\")#[:700]\n",
    "Y_Validation = np.load(r\"C:\\Users\\mikeb\\Desktop\\di_gen\\valid\\data\\train_label_augv2.npy\")\n",
    "s = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(s)\n",
    "X_train = X_train[s]\n",
    "Y_train = Y_train[s]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_Validation.shape)\n",
    "print(Y_Validation.shape)\n",
    "\n",
    "patch_size = 224\n",
    "\n",
    "IMG_WIDTH = patch_size\n",
    "IMG_HEIGHT = patch_size\n",
    "# Number of feature channels \n",
    "INPUT_CHANNELS = 5\n",
    "# Number of output masks (1 in case you predict only one type of objects)\n",
    "OUTPUT_MASK_CHANNELS = 1\n",
    "maxepoch = 300\n",
    "# hyperparameters\n",
    "learning_rate =0.0000359\n",
    "patience = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68e651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the CNN\n",
    "model = UNET_224()\n",
    "model.compile(optimizer=Adam(lr=learning_rate),loss = dice_coef_loss,metrics=[dice_coef,'accuracy'])\n",
    "callbacks = [\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
    "        EarlyStopping(monitor='val_loss', patience=patience, verbose=0),\n",
    "        ModelCheckpoint(r'C:\\Users\\mikeb\\Desktop\\last\\result\\attention2vorg.h5', monitor='val_loss', save_best_only=True, verbose=0),\n",
    "    ]\n",
    "\n",
    "results_03 = model.fit(train_data, train_label, validation_data=(vali_data,vali_label), batch_size=16, epochs=maxepoch, callbacks=callbacks)\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "# save the intermdediate results and training statistics\n",
    "with open(r'C:\\Users\\mikeb\\Desktop\\last\\result\\history_attention_U_netorg.pickle', 'wb') as file_pi:\n",
    "    pickle.dump(results_03.history, file_pi, protocol=2)\n",
    "# save the model5\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the CNN\n",
    "model = UNET_224()\n",
    "model.compile(optimizer=Adam(lr=learning_rate),loss = dice_coef_loss,metrics=[dice_coef,'accuracy'])\n",
    "callbacks = [\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
    "        EarlyStopping(monitor='val_loss', patience=patience, verbose=0),\n",
    "        ModelCheckpoint(r\"C:\\Users\\mikeb\\Desktop\\di_gen\\result\\attention2.h5\", monitor='val_loss', save_best_only=True, verbose=0),\n",
    "    ]\n",
    "\n",
    "results_03 = model.fit(X_train, Y_train, validation_data=(X_Validation,Y_Validation), batch_size=16, epochs=maxepoch, callbacks=callbacks)\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "# save the intermdediate results and training statistics\n",
    "with open(r\"C:\\Users\\mikeb\\Desktop\\di_gen\\result\\historyattention_U_net.pickle\", 'wb') as file_pi:\n",
    "    pickle.dump(results_03.history, file_pi, protocol=2)\n",
    "# save the model\n",
    "model.save(r\"C:\\Users\\mikeb\\Desktop\\di_gen\\result\\attUnet.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights into new model\n",
    "from keras.models import load_model\n",
    "\n",
    "newmodel = load_model(r\"C:\\Users\\mikeb\\Downloads\\attention_U_net_c.h5\", custom_objects={'multiplication': multiplication,'multiplication2': multiplication2,\"dice_coef_loss\":dice_coef_loss, \"dice_coef\":dice_coef})\n",
    "# Reload the model and save the predicted labels.\n",
    "X_test = np.load(r\"C:\\Users\\mikeb\\Desktop\\valid_data\\data\\pred.npy\")\n",
    "preds_test = newmodel.predict(X_test)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
    "# np.save(r\"C:\\Users\\mikeb\\Desktop\\ediited_dem\\pred_result.npy\",preds_test_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "buf =30\n",
    "\n",
    "preds_test_mod = preds_test_t#np.load(r\"C:\\Users\\mikeb\\Desktop\\ediited_dem\\pred_result.npy\")\n",
    "dim = np.load(r\"C:\\Users\\mikeb\\Desktop\\valid_data\\mask.npy\").shape\n",
    "numr = dim[0]//(224 - buf*2)\n",
    "numc = dim[1]//(224 - buf*2)\n",
    "count = -1\n",
    "for i in range(numr):\n",
    "    for j in range(numc):\n",
    "        count += 1    \n",
    "        temp = preds_test_mod[count][buf:-buf,buf:-buf]\n",
    "        if j == 0:\n",
    "            rows = temp\n",
    "        else:\n",
    "            rows = np.concatenate((rows,temp),axis = 1)\n",
    "    if i == 0:\n",
    "        prediction_map = copy.copy(rows)\n",
    "    else:\n",
    "        prediction_map = np.concatenate((prediction_map,rows),axis = 0)\n",
    "prediction_map = prediction_map[:,:,0]\n",
    "# fig= plt.figure(figsize=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ba99c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Generate prediction map\n",
    "# from osgeo import gdal \n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "# mask = np.load(r\"C:\\Users\\mikeb\\Desktop\\last\\masklast.npy\")\n",
    "fig= plt.figure(figsize=(20,15))\n",
    "# prediction_map= cv2.resize(prediction_map, (6075,7850))\n",
    "# Plot the map\n",
    "prediction_map = prediction_map #*mask\n",
    "plt.subplot(1,2, 1)\n",
    "plt.title('result',fontsize = 16)\n",
    "plt.imshow(prediction_map*255,cmap='gray',vmin=0, vmax=1)\n",
    "\n",
    "reference = np.load(r\"C:\\Users\\mikeb\\Desktop\\valid_data\\data\\label.npy\").astype(np.uint8)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Reference map',fontsize = 16)\n",
    "plt.imshow(reference, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aba0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\"\"\"\n",
    "plot_history(): reads Keras result and ogenerate figure of Loss, Dice Coefficient, and Accuracy.\n",
    "\"\"\"\n",
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.keys() if 'loss' in s and 'val' in s]\n",
    "    dice_list = [s for s in history.keys() if 'dice' in s and 'val' not in s]\n",
    "    val_dice_list = [s for s in history.keys() if 'dice' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.subplot(1,3,1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history[l], 'b', label='Training loss (' + str(str(format(history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history[l], 'g', label='Validation loss (' + str(str(format(history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Dice Coefficient\n",
    "    plt.subplot(1,3,2)\n",
    "    for l in dice_list:\n",
    "        plt.plot(epochs, history[l], 'b', label='Training Dice Coefficient (' + str(format(history[l][-1],'.5f'))+')')\n",
    "    for l in val_dice_list:    \n",
    "        plt.plot(epochs, history[l], 'g', label='Validation Dice Coefficient (' + str(format(history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Dice Coefficient')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Dice Coefficient')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.subplot(1,3,3)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history[l], 'b', label='Training accuracy (' + str(format(history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history[l], 'g', label='Validation accuracy (' + str(format(history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aca633",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(r\"C:\\Users\\mikeb\\Downloads\\attention_left.pickle\",'rb')\n",
    "new_dict = pickle.load(infile)\n",
    "infile.close()\n",
    "plt.figure(figsize=(20,5))\n",
    "plot_history(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "buf =30\n",
    "\"\"\"\n",
    "plot_confusion_matrix(): prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.\n",
    "\"\"\"\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# preds_test_mod = np.load(r\"C:\\Users\\mikeb\\Desktop\\last\\test_result.npy\")\n",
    "\n",
    "# dim = np.load(r\"C:\\Users\\mikeb\\Desktop\\last\\ref.npy\").shape\n",
    "# numr = dim[0]//(224 - buf*2)\n",
    "# numc = dim[1]//(224 - buf*2)\n",
    "# count = -1\n",
    "# for i in range(numr):\n",
    "#     for j in range(numc):\n",
    "#         count += 1    \n",
    "#         temp = preds_test_mod[count][buf:-buf,buf:-buf]\n",
    "#         if j == 0:\n",
    "#             rows = temp\n",
    "#         else:\n",
    "#             rows = np.concatenate((rows,temp),axis = 1)\n",
    "#     if i == 0:\n",
    "#         prediction_map = copy.copy(rows)\n",
    "#     else:\n",
    "#         prediction_map = np.concatenate((prediction_map,rows),axis = 0)\n",
    "# prediction_map = prediction_map[:,:,0]\n",
    "# # fig= plt.figure(figsize=(20,15))\n",
    "# # plt.imshow(prediction_map*255,cmap='gray',vmin=0, vmax=255)\n",
    "# # plt.show()\n",
    "\n",
    "# #print(np.unique(prediction_map))\n",
    "\n",
    "# mask\n",
    "mask = np.load(r\"C:\\Users\\mikeb\\Desktop\\valid_data\\mask.npy\")[:prediction_map.shape[0],:prediction_map.shape[1]]\n",
    "[lr,lc] = np.where(mask == 1)\n",
    "\n",
    "# Read reference data\n",
    "groundtruthlist = np.load(r\"C:\\Users\\mikeb\\Desktop\\valid_data\\data\\label.npy\")[:prediction_map.shape[0],:prediction_map.shape[1]][lr,lc]\n",
    "predictionlist = prediction_map[lr,lc]\n",
    "\n",
    "# print(np.unique(groundtruthlist*255))\n",
    "# print(np.unique(predictionlist))\n",
    "# print(type(groundtruthlist))\n",
    "# print(groundtruthlist)\n",
    "# print(type(predictionlist))\n",
    "# print(predictionlist)\n",
    "\n",
    "cm = confusion_matrix((groundtruthlist).astype(int), predictionlist)\n",
    "plot_confusion_matrix(cm,classes=[\"Non-streams\",\"Streams\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e636a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "from sklearn.metrics import f1_score, precision_score,recall_score\n",
    "#print(f1_score(groundtruthlist, predictionlist, average='macro'))\n",
    "print('F1 score of stream: '+str(f1_score(groundtruthlist, predictionlist,pos_label=1)))\n",
    "print('F1 score of nonstream: '+str(f1_score(groundtruthlist, predictionlist,pos_label=0)))\n",
    "print('Precision of stream: '+str(precision_score(groundtruthlist, predictionlist,pos_label=1)))\n",
    "print('Precision of nonstream: '+str(precision_score(groundtruthlist, predictionlist,pos_label=0)))\n",
    "print('Recall of stream: '+str(recall_score(groundtruthlist, predictionlist,pos_label=1)))\n",
    "print('Recall of nonstream: '+str(recall_score(groundtruthlist, predictionlist,pos_label=0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
